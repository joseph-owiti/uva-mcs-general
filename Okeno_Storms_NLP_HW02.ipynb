{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joseph-owiti/uva-mcs-general/blob/develop/Okeno_Storms_NLP_HW02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8joq98nXQwdi"
      },
      "source": [
        "Submission Deadline: __October 6, 2024, 11:59 PM__\n",
        "\n",
        "A penalty will be applied for late submission. Please refer to the course policy for more detail.  \n",
        "\n",
        "## Instructions\n",
        "\n",
        "Please read the instructions carefully before you start working on the homework.\n",
        "\n",
        "- Please follow instructions and printed out the results as required. Keep the printed results and your implementation for grading purpose.\n",
        "    - The TAs will not run your code for grading purpose unless it is necessary. That means, you may lose some points if the printed results are not in the submitted file.\n",
        "- Submission should be via Canvas.\n",
        "    - If you use Google Colab for running the code, please download the file and submit it via Canvas once it's done.\n",
        "    - Submission via a Google Colab link will be considered as an invalid submission.\n",
        "- Please double check the submitted file once you upload it to Canvas.\n",
        "    - Students should be responsible for checking whether they submit the right files.\n",
        "    - Re-submission is not allowed once the deadline is passed.\n",
        "\n",
        "Also, if you missed the class lectures, please study the course materials first before working on the homework. It may save you some time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbi1ud-6beud"
      },
      "source": [
        "# Homework 02 Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kga3AJlUbeuf"
      },
      "source": [
        "### Goal\n",
        "\n",
        "The **goal** of this homework is to provide an opportunity to build an end-to-end system.\n",
        "\n",
        "Specifically, we are going to build a word embedding system, that can\n",
        "\n",
        "1. Read and preprocess raw data\n",
        "2. Use two different ways (latent semantic analysis and skip-gram) to learn word embeddings\n",
        "3. Evaluate the quality of word embeddings using some intrinsic evaluation methods\n",
        "\n",
        "### Submission\n",
        "\n",
        "Your submission should only include this notebook file. Please keep **all the outputs** in your submission for grading. We will run the code only if we are not sure it is correct.\n",
        "\n",
        "### Dependency\n",
        "\n",
        "You will need the following package to finish this homework assignment\n",
        "\n",
        "- [spaCy](https://pypi.org/project/spacy/)\n",
        "- [fasttext](https://pypi.org/project/fasttext/)\n",
        "\n",
        "### Hint\n",
        "\n",
        "Search for the keyword `TODO` to find out which parts need your input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foccJkL4beug",
        "outputId": "9f63a02c-e4a9-469c-ad72-01024ceca230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading ...\n",
            "Decompressing the file ...\n",
            "Archive:  embeddings.zip\n",
            "   creating: embeddings/\n",
            "  inflating: embeddings/imdb-small.txt  \n",
            "  inflating: embeddings/word-pairs.txt  \n",
            "Read 10000 sentences\n"
          ]
        }
      ],
      "source": [
        "# Download the data from course webpage\n",
        "import urllib.request\n",
        "from os.path import isfile\n",
        "if not isfile(\"embeddings/imdb-small.txt\"):\n",
        "    url = \"https://yangfengji.net/uva-nlp-grad/data/embeddings.zip\"\n",
        "    print(\"Downloading ...\")\n",
        "    filename, headers = urllib.request.urlretrieve(url, filename=\"embeddings.zip\")\n",
        "\n",
        "    print(\"Decompressing the file ...\")\n",
        "    !unzip embeddings.zip\n",
        "\n",
        "sents = open(\"embeddings/imdb-small.txt\").read().split(\"\\n\")\n",
        "print(\"Read {} sentences\".format(len(sents)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C2HysKMbeuh"
      },
      "source": [
        "## 1. Data Processing (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv9x86d-beuh"
      },
      "source": [
        "Data processing is an **essential** skill for NLP researchers. Unlike machine learning where researchers sometimes may want to use synthetic data to demonstrate the potential of their algorithms, NLP researchers need to deal with real-world data all the time. Unfortunately, this means that these data are noisy and often contain irregular patterns. Therefore, a reasonable data processing can alleviate the challenge of building NLP systems to some extent and may also help boost the performance of machine learning models.\n",
        "\n",
        "Data processing for learning word embeddings includes two basic modules\n",
        "\n",
        "- Tokenizing texts and replacing some special tokens\n",
        "- Filtering low-frequency and building a vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqAnj84Tbeuh"
      },
      "source": [
        "### 1.1 Tokenization (2 points)\n",
        "\n",
        "The following function *tokenize()* should include the following components\n",
        "\n",
        "1. Load the raw text from the file named **imdb-small.txt**\n",
        "2. Convert all characters into lower cases\n",
        "3. Tokenize the raw text using `spaCy`\n",
        "4. Remove all punctuation (as single tokens) and replace all numbers (as single tokens) with a special token `<num>`\n",
        "5. Write the preprocessed text to the file named **imdb-small.txt.tokenized** and maintain the same format (one paragraph per line)\n",
        "\n",
        "(The file names are pre-defined, please do not change them.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ9dtItlbeuh"
      },
      "outputs": [],
      "source": [
        "# TODO: add necessary packages here\n",
        "from spacy.lang.en import English\n",
        "import re\n",
        "import string\n",
        "\n",
        "def tokenize(infname=\"embeddings/imdb-small.txt\"):\n",
        "    outfname = open(infname + \".tokenized\", \"w\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # TODO: add your code here\n",
        "\n",
        "    # Load and read the raw text\n",
        "    with open(infname, 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Convert raw text into lowercase characters and split into paragraphs\n",
        "    text = text.lower()\n",
        "    paragraphs = text.split('\\n')\n",
        "\n",
        "    # Tokenize the paragraphs using spaCy\n",
        "    nlp = English()\n",
        "    tokenize = nlp.tokenizer\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if paragraph.strip():\n",
        "            tokens = [token.text for token in tokenize(paragraph)]\n",
        "\n",
        "            # Clean up tokens\n",
        "            new_tokens = []\n",
        "            for token in tokens:\n",
        "                if token.isalpha():\n",
        "                    new_tokens.append(token)\n",
        "                elif token.isdigit():\n",
        "                    new_tokens.append(\"<num>\")\n",
        "                elif token in string.punctuation:\n",
        "                    continue\n",
        "                else:\n",
        "                    new_tokens.append(token)\n",
        "\n",
        "            # Write the preprocessed paragraph to the file\n",
        "            outfname.write(\" \".join(new_tokens) + \"\\n\")  # Write each paragraph on a new line\n",
        "\n",
        "    # ----------------------------------------\n",
        "\n",
        "    outfname.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-yYsfWmbeui"
      },
      "source": [
        "### 1.2 Filtering (2 points)\n",
        "\n",
        "The following function *token_filter()* should include the following components\n",
        "\n",
        "1. Remove the words that appear in the data less than 5 times (word_frequency < 5)\n",
        "2. Write the filtered data to the file named **imdb-small.txt.filtered** and maintain the same format (one sentence per line)\n",
        "3. Return a Python list that contains all the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "bcWBcJoSbeui"
      },
      "outputs": [],
      "source": [
        "# TODO: add necessary packages here\n",
        "from collections import Counter\n",
        "\n",
        "def token_filter(infname=\"embeddings/imdb-small.txt.tokenized\", thresh=5):\n",
        "    outfname = open(infname.replace(\".tokenized\", \".filtered\"), 'w')\n",
        "    vocab = []\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # TODO: remove \"pass\" and add your code here\n",
        "\n",
        "    with open(infname, 'r') as f:\n",
        "        text = f.read()\n",
        "    print(\"The original vocab size = %d\" % len(text))\n",
        "\n",
        "    paragraphs = text.split('\\n')\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        word_counts = Counter(paragraph.split())\n",
        "        new_paragraphs = \"\"\n",
        "        for word, count in word_counts.items():\n",
        "            if count > 5:\n",
        "                vocab.append(word)\n",
        "                new_paragraphs += word + \" \"\n",
        "\n",
        "        outfname.write(\" \".join(new_paragraphs) + \"\\n\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "\n",
        "    outfname.close()\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CZox3VFbeui"
      },
      "source": [
        "### 1.3 Put all together (1 point)\n",
        "\n",
        "The following code block will call the previous two functions to do data preprocessing.\n",
        "\n",
        "This code block should include the following steps\n",
        "\n",
        "- tokenization\n",
        "- build the vocabulary with the variable name `vocab`\n",
        "- print out the size of the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "syFxe2uobeui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddc04ec-4026-4eb3-b169-69e0db53b9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original vocab size = 12687974\n",
            "The final vocab size = 50550\n"
          ]
        }
      ],
      "source": [
        "tokenize()\n",
        "vocab = token_filter()\n",
        "print(\"The final vocab size = {}\".format(len(vocab)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMVhCnpsbeui"
      },
      "source": [
        "## 2. Word Embeddings (5 points)\n",
        "\n",
        "In this section, you need to implement two different ways of constructing word embeddings: latent semantic analysis  and skipgram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptLnV4mHbeuj"
      },
      "source": [
        "### 2.1 Latent semantic analysis (3 points)\n",
        "\n",
        "The function of LSA should include the following components\n",
        "\n",
        "- Construct the word-doc matrix using `CountVectorizer` with `tokenizer=lambda x : x.split()`, make sure in this matrix that each row represents one word and each column represents one document (sentence, to be accurate in this case)\n",
        "- Use the `TruncatedSVD` from `sklearn.decomposition` to factorize the word-doc matrix\n",
        "- Construct the word embedding matrix with dimensionality as $v \\times k$, where $v$ is the vocab size and $k$ is the word embedding dimension\n",
        "\n",
        "The LSA() function should return\n",
        "\n",
        "- **embeddings**: A matrix with size $v\\times k$ that contains all the word embeddings\n",
        "- **vocab**: A Python dict with size $v$ that maps a word to the corresponding word index. Please pay attention to the mapping relation in vocab, which will be needed in the evaluation section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "5zI4dJnFbeuj"
      },
      "outputs": [],
      "source": [
        "# TODO: add necessary packages here\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def LSA(fname = \"embeddings/imdb-small.txt.filtered\", dim=50):\n",
        "    sents = open(fname).read().split(\"\\n\")\n",
        "\n",
        "    print(\"Read {} sentences\".format(len(sents)))\n",
        "\n",
        "\n",
        "    # -------------------------------------\n",
        "    # TODO: add your code here\n",
        "\n",
        "    vectorizer = CountVectorizer(tokenizer=lambda x : x.split())\n",
        "    word_doc_matrix = vectorizer.fit_transform(sents)\n",
        "\n",
        "    svd = TruncatedSVD(n_components=dim)\n",
        "    word_doc_matrix_reduced = svd.fit_transform(word_doc_matrix)\n",
        "\n",
        "    embeddings = word_doc_matrix_reduced\n",
        "    vocab = vectorizer.vocabulary_\n",
        "    print(\"The LSA matrix shape = {}\".format(embeddings.shape))\n",
        "    print(\"The LSA vocab size = {}\".format(len(vocab)))\n",
        "\n",
        "    # -------------------------------------\n",
        "    return embeddings, vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v17bhMzSbeuj"
      },
      "source": [
        "### 2.2 Skip-gram model (2 points)\n",
        "\n",
        "In this section, you do not have to implement the skip-gram model by yourself. An authentic implementation of skip-gram can be found in the Python package [fasttext](https://pypi.org/project/fasttext/), which you can install on the your local machine with the folllwing commandline or directly load the package if you are using Google Colab.\n",
        "```python\n",
        "pip install fasttext\n",
        "```\n",
        "\n",
        "In the following code, please use the `fasttext.train_unsupervised` function for the skipgram() implementation. For the `fasttext.train_unsupervised`, please use the following configurations\n",
        "\n",
        "- `model='skipgram'`\n",
        "- Context window size: `ws = 3`\n",
        "- Word embedding dimension: `dim = 50`\n",
        "- Number of negative examples: `neg = 5`\n",
        "\n",
        "For all other parameters, use their default values.\n",
        "\n",
        "Similar to the previous LSA(), Skipgram() should return\n",
        "\n",
        "- **embeddings**: A matrix with size $v\\times k$ that contains all the word embeddings\n",
        "- **vocab**: A Python dict with size $v$ that maps an index to the corresponding word\n",
        "\n",
        "To get the word embeddings and vocab from fasttext, you need to understand [some functions](https://pypi.org/project/fasttext/#api) provided by the `model` object in the fasttext."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_kuAPTY5WeD",
        "outputId": "15c08555-782a-4dac-b1ff-b04d9c3b8b1c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296183 sha256=ec4d15bb61f5514da8b40c7fdda1bad7648e76bed457230917c8ddd277ad0dd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "cQ0arCTEbeuj"
      },
      "outputs": [],
      "source": [
        "# TODO: add necessary packages here\n",
        "import fasttext\n",
        "\n",
        "def Skipgram(fname = \"embeddings/imdb-small.txt.filtered\", ws=3, dim=50):\n",
        "    # ------------------------------------------\n",
        "    # TODO: add your code here\n",
        "    model = fasttext.train_unsupervised(fname, model='skipgram', ws=ws, dim=dim, neg=5)\n",
        "\n",
        "    embeddings = model.get_output_matrix()\n",
        "    vocab = model.get_words()\n",
        "    print(\"The Skipgram matrix shape = {}\".format(embeddings.shape))\n",
        "    print(\"The Skipgram vocab size = {}\".format(len(vocab)))\n",
        "\n",
        "    # ------------------------------------------\n",
        "    return embeddings, vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPy2ImAlbeuj"
      },
      "source": [
        "### 2.3 Put all together\n",
        "\n",
        "Run the following code blocks to get word embeddings from two different methods. It may take a couple of minutes to compute both embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Ahi-Z01zbeuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c58278-8889-4185-a2c3-62db1ee9d241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 10001 sentences\n",
            "The LSA matrix shape = (10001, 50)\n",
            "The LSA vocab size = 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Skipgram matrix shape = (34, 50)\n",
            "The Skipgram vocab size = 34\n"
          ]
        }
      ],
      "source": [
        "embeddings_lsa, vocab_lsa = LSA()\n",
        "embeddings_sg, vocab_sg = Skipgram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKbl52S2beuj"
      },
      "source": [
        "The following code will serve as the sanity check that `vocab_lsa` and `vocab_sg` contain the same words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "abTUwVVRbeuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cc30ef-4822-4460-dacb-1c1aac145f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The word that only appear in one vocab: {'–', 'b', '<', 'ä', 'á', '2', 'g', 'h', 'n', '>', ')', 'u', 'ö', 's', 'q', 'k', '.', 'l', 'p', '7', '·', 'w', 'a', 'z', \"'\", '-', '’', 'ç', '\\x96', 'j', 'i', 'v', 'y', 'x', 'o', 'd', 'm', 'f', '»', '\\x91', 'ü', 'r', '1', ':', '3', 'e', 'é', '«', 'c'}\n"
          ]
        }
      ],
      "source": [
        "lsa_word_set = set([item[0] for item in vocab_lsa.items()])\n",
        "sg_word_set = set([item[0] for item in vocab_sg[0]])\n",
        "sym_diff = lsa_word_set.symmetric_difference(sg_word_set)\n",
        "\n",
        "if len(sym_diff) == 0:\n",
        "    print(\"vocab_lsa and vocab_sg contain the same words!\")\n",
        "else:\n",
        "    print(\"The word that only appear in one vocab: {}\".format(sym_diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyl_XqYlbeuj"
      },
      "source": [
        "If the only word from the `symmetric_difference()` function is `</s>`, then your implementation should be fine. (`</s>` was added by `fasttext` automatically to the end of each text.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3av8qPGMbeuj"
      },
      "source": [
        "## 3. Evaluation (5 points)\n",
        "\n",
        "In this homework, we will only use intrinsic evaluation. Specifically, for a list of predefined word pairs with their similarity scores, the evaluation is to calculate the correlation between the predefined similarity scores and the cosine similarity scores based on word embeddings. The higher the correlation, the better the quality of word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLlF2ZcBbeuk"
      },
      "outputs": [],
      "source": [
        "def load_wordpairs(fname = \"embeddings/word-pairs.txt\", vocab=None):\n",
        "    records = {}\n",
        "    with open(fname) as fin:\n",
        "        for line in fin:\n",
        "            items = line.strip().split(\",\")\n",
        "            if (items[1] in vocab) and (items[2] in vocab): # make sure both words in the vocab\n",
        "                records[(items[1],items[2])] = float(items[3])\n",
        "    print(\"Load {} pairs of words for evaluation\".format(len(records)))\n",
        "    return records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP9rT0Jfbeuk"
      },
      "source": [
        "### 3.1 Word similarity correlation (2 points)\n",
        "\n",
        "The purpose of this section is to implement the correlation function that compares the predefined scores and the scores computed by cosine similarity. The code of the correlation function is almost done, and the only thing left is the code for computing cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tazeb8dibeuk"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "# TODO: Add necessary packages here\n",
        "\n",
        "def correlation(records, embeddings, vocab):\n",
        "    predefined_scores = []\n",
        "    cossim_scores = []\n",
        "    for (words, sim_score) in records.items():\n",
        "        predefined_scores.append(sim_score)\n",
        "        # ---------------------------------\n",
        "        # TODO: add your code here for computing the cossine similarity\n",
        "        #       between words[0] and words[1], and assign the value to variable \"score\"\n",
        "\n",
        "        # ---------------------------------\n",
        "    corr = pearsonr(predefined_scores, cossim_scores)\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzFS-wdnbeuk"
      },
      "source": [
        "Run the following code block to calculate the correlations between pre-defined similarity scores and the cosine similarity scores based on word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzFQqamVbeuk"
      },
      "outputs": [],
      "source": [
        "records_lsa = load_wordpairs(vocab=vocab_lsa)\n",
        "corr_lsa = correlation(records_lsa, embeddings_lsa, vocab_lsa)\n",
        "print(\"The correlation with the LSA embeddings = {} with p-value {}\".format(corr_lsa[0], corr_lsa[1]))\n",
        "records_sg = load_wordpairs(vocab=vocab_sg)\n",
        "corr_sg = correlation(records_sg, embeddings_sg, vocab_sg)\n",
        "print(\"The correlation with the LSA embeddings = {} with p-value {}\".format(corr_sg[0], corr_sg[1]))\n",
        "\n",
        "if corr_lsa[0] > corr_sg[0]:\n",
        "    print(\"LSA is better than Skip-gram\")\n",
        "elif corr_lsa[0] < corr_sg[0]:\n",
        "    print(\"Skipgram is better than LSA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWOW0SZlbeuk"
      },
      "source": [
        "### 3.2 Analysis of context window size in Skipgram (3 points)\n",
        "\n",
        "With the correlation function, we can analyze the effect of different context window sizes in the Skipgram model. Specifically, please call the previous implementation\n",
        "\n",
        "- `Skipgram(fname, ws, dim=50)` with the context window size `ws` as 3, 6, 9, 12, 15\n",
        "- For each context window size, calculate the correlation using the function `correlation(records, embeddings, vocab)`\n",
        "- **Print out** the fives correlation scores in your final submission: one score per line with the following format\n",
        "<center> ws\\t correlation</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tv-wIFibeuk"
      },
      "outputs": [],
      "source": [
        "# TODO: add your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sr9rmhRbeuk"
      },
      "source": [
        "Similar experiment can also be conducted on the parameter of negative examples `neg`, but it will not be included in this homework."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}